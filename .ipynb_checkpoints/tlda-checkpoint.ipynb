{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>TLDA</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tlda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `theta_u` : pattern distribution of user *u*.\n",
    "2. `phi_t` : pattern distribution of time *t*.\n",
    "3. `psi_z` : venue category distribution of pattern *z*\n",
    "4. sfs\n",
    "\n",
    "Analogous to the traditional LDA : `document` - `user/time`, `topic` - `cultural pattern`, and `word` - `venue`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Extraction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `checkin_data` : {*user1* : [(*venue_category_1*, *time_1*), (*venue_category_2*, *time_2*),..., (*venue_category_n*, *time_n*)],*user2* : [(*venue_category_1*, *time_1*), (*venue_category_2*, *time_2*) ...], *user3*...}.\n",
    "2. `venues` : list of venue ids.\n",
    "3. `venue_categories` : list of venue categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month(str): Jan Feb [Mar not included] Apr May Jun Jul Aug Sep Oct Nov Dec\n",
    "# day: Mon, Tue, Wed, Thu, Fri, Sat, Sun\n",
    "# Hour(int): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23\n",
    "\n",
    "\n",
    "# data format: (user1 (venue_category_1, time_1), (venue_category_2, time_2), (venue_category_3, time_3))\n",
    "# time format: month-day-hour; Example: (Oct-Wed-13)\n",
    "\n",
    "# our main data\n",
    "checkin_data = defaultdict(list)\n",
    "\n",
    "# store all venues\n",
    "venues = set()\n",
    "\n",
    "# The encoding problem does not exist in windows OS\n",
    "nyc = open(\"nyc.txt\", encoding='ISO-8859-1')\n",
    "\n",
    "i = 0\n",
    "\n",
    "# data extraction\n",
    "for checkin in nyc:\n",
    "    checkin_time = (checkin.split('\\t')[-1]).split(' ')\n",
    "    \n",
    "    # venues id (not used)\n",
    "    venues.add(checkin.split('\\t')[1])\n",
    "    \n",
    "    # time in the correct format\n",
    "    time = checkin_time[1] + '-' + checkin_time[0] + '-' + checkin_time[3].split(':')[0]\n",
    "    \n",
    "    # corresponding venue category, combine categories into a single category\n",
    "    category = checkin.split('\\t')[3]\n",
    "    if 'Restaurant' in category:\n",
    "        venue_category = 'Restaurant'\n",
    "    elif 'Joint' in category:\n",
    "        venue_category = 'Food Joint'\n",
    "    elif 'Museum' in category:\n",
    "        venue_category = 'Museum'\n",
    "    else:\n",
    "        venue_category = category\n",
    "    \n",
    "    # one checkin tuple of this user\n",
    "    single_checkin = (venue_category, time)\n",
    "\n",
    "    #user id\n",
    "    user = checkin.split('\\t')[0]\n",
    "    \n",
    "    checkin_data[user].append(single_checkin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format : {venue_category_1 : count 1, venue_category_2 : count 2, ...}\n",
    "venue_count = defaultdict(lambda : 0)\n",
    "for user, checkins in checkin_data.items():\n",
    "    for checkin in checkins:\n",
    "        venue_count[checkin[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all checkins whose venue categories have not been visited over 1200 times\n",
    "for user, checkins in checkin_data.items():\n",
    "    new_checkins = []\n",
    "    for checkin in checkins:\n",
    "        if venue_count[checkin[0]] >= 1200:\n",
    "            new_checkins.append(checkin)\n",
    "    checkin_data[user] = new_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove users who visited less than 100 places\n",
    "remove = []\n",
    "for user, checkins in checkin_data.items():\n",
    "    if len(checkins) < 100:\n",
    "        remove.append(user)\n",
    "for r in remove:\n",
    "    del checkin_data[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 782\n",
      "Number of venue categories: 40\n",
      "Nnumber of venues: 38333\n",
      "Number of checkins: 158746\n",
      "Maxmimum number of check-ins per user: 2063\n",
      "Minimum number of check-ins per user: 100\n",
      "Number of distinct check-ins: 39936\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of users: {}\".format(len(checkin_data)))\n",
    "venue_categories = set()\n",
    "\n",
    "for user, checkins in checkin_data.items():\n",
    "    for checkin in checkins:\n",
    "        venue_categories.add(checkin[0])\n",
    "print(\"Number of venue categories: {}\".format(len(venue_categories)))\n",
    "\n",
    "total_checkins = 0\n",
    "for user, checkins in checkin_data.items():\n",
    "    total_checkins += len(checkins)\n",
    "\n",
    "print(\"Nnumber of venues: {}\".format(len(venues)))\n",
    "    \n",
    "print(\"Number of checkins: {}\".format(total_checkins))\n",
    "\n",
    "max = 0\n",
    "min = 5000\n",
    "for user, checkins in checkin_data.items():\n",
    "    if len(checkins) <= min:\n",
    "        min = len(checkins)\n",
    "    if len(checkins) >= max:\n",
    "        max = len(checkins)\n",
    "print(\"Maxmimum number of check-ins per user: {}\".format(max))\n",
    "print(\"Minimum number of check-ins per user: {}\".format(min))\n",
    "\n",
    "distinct_checkins = set()\n",
    "for checkins in checkin_data.values():\n",
    "    for checkin in checkins:\n",
    "        distinct_checkins.add(checkin)\n",
    "print(\"Number of distinct check-ins: {}\".format(len(distinct_checkins)))\n",
    "\n",
    "# store categories into a file\n",
    "f = open('venue_categories.txt', 'w+')\n",
    "\n",
    "for v in venue_categories:\n",
    "    f.write(v + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_users = len(checkin_data)\n",
    "\n",
    "number_of_venue_categories = len(venue_categories)\n",
    "\n",
    "number_of_distinct_checkins = len(distinct_checkins)\n",
    "\n",
    "number_of_checkins = total_checkins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Basic info - Before </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `1083` users, with id from `1` to `1083`. \n",
    "2. `38333` venues.\n",
    "3. `251` venue categories.\n",
    "4. `227428` checkins\n",
    "5. Maxmimum number of check-ins per user: `2697`\n",
    "6. Minimum number of check-ins per user: `100`\n",
    "7. Number of distinct check-ins: `81320`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic info - After</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `782` users\n",
    "2. `38333` venues.\n",
    "3. `251` venue categories.\n",
    "4. `158746` checkins\n",
    "5. Maxmimum number of check-ins per user: `2063`\n",
    "6. Minimum number of check-ins per user: `100`\n",
    "7. Number of distinct check-ins: `39936`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What the author did</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. first filter cultural fans based on users with at least 20 check-ins. \n",
    "2. Besides a venue category label, also represent a temporal label for each cultural check-in with three levels of identifiers, including month of year (Oct), day of week (Fri), and hour of day (13). Following this form of expression, a user’s check-in history can be represented as `(User3, ((Concert hall, JulFri20), (Golf, OctSun10), (Yoga, AprFri18))`, for example.\n",
    "3. Run the TLDA model with the optimum number of patterns *K* given by TCV. We adopt 7 numbers from 3 to 9 as candidates, run the TLDA for 100 iterations each, and get their respective average TCV scores. Select *K* with the highest score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> What we did with respect to points above</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For our data set, the minimum number of checkins per user is 100 (the maximum is 2063), therefore, there is no need to trim the data.\n",
    "2. We followed the author's approach, however, based on the heatmap, the cultural pattern with respect to day is not signification, therefore, we might change the time format to `(month-hour)` instand of `(month-day-hour)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> My approach </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to `lda` library has to be a document-term matrix `X` where `X_{ij}` = the number of times term at index `j` appears in document `i`. <br>\n",
    "There are 782 users, in other words, 782 \"documents\", and 39936 distinct check-in data, in other words, 39936 \"words\". So we construct a 782 by 39936 matrix as the input. <br>\n",
    "Because users in the `checkin_data` is not ordered based on their ids, we have to create a user_id - index mapping. Similarily, we also create a checkin-index mapping. <br>\n",
    "Then, after the LDA, we also need to map index back to users and checkins. Therefore, we also need a index-user_id mapping and index-checkin mapping which is exactly the reverse of two data structures above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Potentianl issues </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we adopt the notion: `user` - `document`, `cultural pattern` - `topic`, `checkin` - `words`. \n",
    "1. The issue concerns me the most this the low frequencies of all words. Within a document, it is less likely (stil possible) to have the same word occurs twice. Within the corpus which consists of 1083 documents, on average, a word appears four times cross these 1083 documents. This is not a very good ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Construct the user_id and checkin mappings </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkin_data : {user1 : [(venue_category_1, time_1), (venue_category_2, time_2), (venue_category_3, time_3...)], \n",
    "#                user2 : [(venue_category_13, time_13), (...) ...] ...}\n",
    "\n",
    "\n",
    "user_index_mapping = dict()\n",
    "\n",
    "index_user_mapping = dict()\n",
    "\n",
    "for index, user in enumerate(checkin_data.keys()):\n",
    "    user_index_mapping[user] = index\n",
    "    index_user_mapping[index] = user\n",
    "\n",
    "    \n",
    "checkin_index_mapping = dict()\n",
    "\n",
    "index_checkin_mapping = dict()\n",
    "\n",
    "index = 0\n",
    "for checkins in checkin_data.values():\n",
    "    for checkin in checkins:\n",
    "        if checkin_index_mapping.get(checkin) == None:\n",
    "            checkin_index_mapping[checkin] = index\n",
    "            index_checkin_mapping[index] = checkin\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Main user-checkin matrix </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and initialize the main matrix with all elements equal to zero\n",
    "main_matrix = [[0] * number_of_distinct_checkins] * number_of_users\n",
    "main_matrix = np.array(main_matrix)\n",
    "\n",
    "# fill in the main matrix with data, suppose user1 has checkin data (venue_category_1, time_1), then element of main matrix at \n",
    "# index i = index_user_mapping[user1], j = index_checkin_mapping[(venue_category_1, time_1)] is 1\n",
    "for user, checkins in checkin_data.items():\n",
    "    i = user_index_mapping[user]\n",
    "    for checkin in checkins:\n",
    "        j = checkin_index_mapping[checkin]\n",
    "        main_matrix[i,j] += 1 # it is possible that the same time occurs twice!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "161\n",
      "--------------------------------\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check if code is correct\n",
    "index = 123\n",
    "print(np.sum(main_matrix[index]))\n",
    "\n",
    "user = index_user_mapping[index]\n",
    "print(len(checkin_data[user]))\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "l1 = list()\n",
    "l2 = set() # because there are duplicates\n",
    "x = 0\n",
    "for i in main_matrix[index]:\n",
    "    if i != 0:\n",
    "        l1.append(x)\n",
    "    x += 1\n",
    "\n",
    "for c in checkin_data[user]:\n",
    "    l2.add(checkin_index_mapping[c])\n",
    "    \n",
    "l2 = list(l2)\n",
    "print(sorted(l1) == sorted(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> venue_category, month, day and hour mappings </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_categories = set()\n",
    "for user, checkins in checkin_data.items():\n",
    "    for checkin in checkins:\n",
    "        venue_categories.add(checkin[0])\n",
    "        \n",
    "venue_index_mapping = dict()\n",
    "index_venue_mapping = dict()\n",
    "\n",
    "for i, venue in enumerate(venue_categories):\n",
    "    venue_index_mapping[venue] = i\n",
    "    index_venue_mapping[i] = venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE IS NO MARCH IN THE DATA SET!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "month = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_index_mapping = dict()\n",
    "index_month_mapping = dict()\n",
    "\n",
    "for i, m in enumerate(month):\n",
    "    month_index_mapping[m] = i\n",
    "    index_month_mapping[i] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_index_mapping = dict()\n",
    "index_day_mapping = dict()\n",
    "\n",
    "for i, d in enumerate(day):\n",
    "    day_index_mapping[d] = i\n",
    "    index_day_mapping[i] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', \n",
    "        '20', '21', '22', '23']\n",
    "hour_index_mapping = dict()\n",
    "index_hour_mapping = dict()\n",
    "\n",
    "for i, h in enumerate(hour):\n",
    "    hour_index_mapping[h] = i\n",
    "    index_hour_mapping[i] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pass the matrix to the algorithm </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 782\n",
      "INFO:lda:vocab_size: 39936\n",
      "INFO:lda:n_words: 158746\n",
      "INFO:lda:n_topics: 20\n",
      "INFO:lda:n_iter: 2000\n",
      "INFO:lda:<0> log likelihood: -2401218\n",
      "INFO:lda:<10> log likelihood: -1850445\n",
      "INFO:lda:<20> log likelihood: -1811113\n",
      "INFO:lda:<30> log likelihood: -1782690\n",
      "INFO:lda:<40> log likelihood: -1762112\n",
      "INFO:lda:<50> log likelihood: -1746321\n",
      "INFO:lda:<60> log likelihood: -1735341\n",
      "INFO:lda:<70> log likelihood: -1726853\n",
      "INFO:lda:<80> log likelihood: -1719612\n",
      "INFO:lda:<90> log likelihood: -1714762\n",
      "INFO:lda:<100> log likelihood: -1710290\n",
      "INFO:lda:<110> log likelihood: -1706043\n",
      "INFO:lda:<120> log likelihood: -1702054\n",
      "INFO:lda:<130> log likelihood: -1699030\n",
      "INFO:lda:<140> log likelihood: -1697395\n",
      "INFO:lda:<150> log likelihood: -1695106\n",
      "INFO:lda:<160> log likelihood: -1693093\n",
      "INFO:lda:<170> log likelihood: -1691611\n",
      "INFO:lda:<180> log likelihood: -1690088\n",
      "INFO:lda:<190> log likelihood: -1689211\n",
      "INFO:lda:<200> log likelihood: -1688028\n",
      "INFO:lda:<210> log likelihood: -1686980\n",
      "INFO:lda:<220> log likelihood: -1686234\n",
      "INFO:lda:<230> log likelihood: -1685566\n",
      "INFO:lda:<240> log likelihood: -1685229\n",
      "INFO:lda:<250> log likelihood: -1684617\n",
      "INFO:lda:<260> log likelihood: -1684188\n",
      "INFO:lda:<270> log likelihood: -1684118\n",
      "INFO:lda:<280> log likelihood: -1682826\n",
      "INFO:lda:<290> log likelihood: -1682931\n",
      "INFO:lda:<300> log likelihood: -1682833\n",
      "INFO:lda:<310> log likelihood: -1682545\n",
      "INFO:lda:<320> log likelihood: -1681873\n",
      "INFO:lda:<330> log likelihood: -1681949\n",
      "INFO:lda:<340> log likelihood: -1681140\n",
      "INFO:lda:<350> log likelihood: -1680583\n",
      "INFO:lda:<360> log likelihood: -1680918\n",
      "INFO:lda:<370> log likelihood: -1681021\n",
      "INFO:lda:<380> log likelihood: -1680094\n",
      "INFO:lda:<390> log likelihood: -1679578\n",
      "INFO:lda:<400> log likelihood: -1680160\n",
      "INFO:lda:<410> log likelihood: -1679854\n",
      "INFO:lda:<420> log likelihood: -1680428\n",
      "INFO:lda:<430> log likelihood: -1680793\n",
      "INFO:lda:<440> log likelihood: -1680554\n",
      "INFO:lda:<450> log likelihood: -1679897\n",
      "INFO:lda:<460> log likelihood: -1679648\n",
      "INFO:lda:<470> log likelihood: -1679171\n",
      "INFO:lda:<480> log likelihood: -1678634\n",
      "INFO:lda:<490> log likelihood: -1679052\n",
      "INFO:lda:<500> log likelihood: -1679032\n",
      "INFO:lda:<510> log likelihood: -1678583\n",
      "INFO:lda:<520> log likelihood: -1677970\n",
      "INFO:lda:<530> log likelihood: -1678599\n",
      "INFO:lda:<540> log likelihood: -1678305\n",
      "INFO:lda:<550> log likelihood: -1678454\n",
      "INFO:lda:<560> log likelihood: -1678745\n",
      "INFO:lda:<570> log likelihood: -1678812\n",
      "INFO:lda:<580> log likelihood: -1678619\n",
      "INFO:lda:<590> log likelihood: -1678395\n",
      "INFO:lda:<600> log likelihood: -1677876\n",
      "INFO:lda:<610> log likelihood: -1678109\n",
      "INFO:lda:<620> log likelihood: -1678211\n",
      "INFO:lda:<630> log likelihood: -1678062\n",
      "INFO:lda:<640> log likelihood: -1678330\n",
      "INFO:lda:<650> log likelihood: -1678248\n",
      "INFO:lda:<660> log likelihood: -1677582\n",
      "INFO:lda:<670> log likelihood: -1677850\n",
      "INFO:lda:<680> log likelihood: -1678129\n",
      "INFO:lda:<690> log likelihood: -1677952\n",
      "INFO:lda:<700> log likelihood: -1677727\n",
      "INFO:lda:<710> log likelihood: -1677311\n",
      "INFO:lda:<720> log likelihood: -1677468\n",
      "INFO:lda:<730> log likelihood: -1677750\n",
      "INFO:lda:<740> log likelihood: -1677562\n",
      "INFO:lda:<750> log likelihood: -1677793\n",
      "INFO:lda:<760> log likelihood: -1677506\n",
      "INFO:lda:<770> log likelihood: -1677324\n",
      "INFO:lda:<780> log likelihood: -1677639\n",
      "INFO:lda:<790> log likelihood: -1676925\n",
      "INFO:lda:<800> log likelihood: -1676831\n",
      "INFO:lda:<810> log likelihood: -1677442\n",
      "INFO:lda:<820> log likelihood: -1676758\n",
      "INFO:lda:<830> log likelihood: -1677468\n",
      "INFO:lda:<840> log likelihood: -1677622\n",
      "INFO:lda:<850> log likelihood: -1677406\n",
      "INFO:lda:<860> log likelihood: -1677963\n",
      "INFO:lda:<870> log likelihood: -1677910\n",
      "INFO:lda:<880> log likelihood: -1677273\n",
      "INFO:lda:<890> log likelihood: -1677418\n",
      "INFO:lda:<900> log likelihood: -1676936\n",
      "INFO:lda:<910> log likelihood: -1677667\n",
      "INFO:lda:<920> log likelihood: -1677212\n",
      "INFO:lda:<930> log likelihood: -1677352\n",
      "INFO:lda:<940> log likelihood: -1678287\n",
      "INFO:lda:<950> log likelihood: -1677618\n",
      "INFO:lda:<960> log likelihood: -1677867\n",
      "INFO:lda:<970> log likelihood: -1677436\n",
      "INFO:lda:<980> log likelihood: -1677501\n",
      "INFO:lda:<990> log likelihood: -1677377\n",
      "INFO:lda:<1000> log likelihood: -1677240\n",
      "INFO:lda:<1010> log likelihood: -1677546\n",
      "INFO:lda:<1020> log likelihood: -1677557\n",
      "INFO:lda:<1030> log likelihood: -1677911\n",
      "INFO:lda:<1040> log likelihood: -1677113\n",
      "INFO:lda:<1050> log likelihood: -1676783\n",
      "INFO:lda:<1060> log likelihood: -1677277\n",
      "INFO:lda:<1070> log likelihood: -1678281\n",
      "INFO:lda:<1080> log likelihood: -1677380\n",
      "INFO:lda:<1090> log likelihood: -1677009\n",
      "INFO:lda:<1100> log likelihood: -1677338\n",
      "INFO:lda:<1110> log likelihood: -1677028\n",
      "INFO:lda:<1120> log likelihood: -1677351\n",
      "INFO:lda:<1130> log likelihood: -1677100\n",
      "INFO:lda:<1140> log likelihood: -1676723\n",
      "INFO:lda:<1150> log likelihood: -1677029\n",
      "INFO:lda:<1160> log likelihood: -1676899\n",
      "INFO:lda:<1170> log likelihood: -1676670\n",
      "INFO:lda:<1180> log likelihood: -1676881\n",
      "INFO:lda:<1190> log likelihood: -1676735\n",
      "INFO:lda:<1200> log likelihood: -1676667\n",
      "INFO:lda:<1210> log likelihood: -1676414\n",
      "INFO:lda:<1220> log likelihood: -1676190\n",
      "INFO:lda:<1230> log likelihood: -1676300\n",
      "INFO:lda:<1240> log likelihood: -1676826\n",
      "INFO:lda:<1250> log likelihood: -1676431\n",
      "INFO:lda:<1260> log likelihood: -1676219\n",
      "INFO:lda:<1270> log likelihood: -1676376\n",
      "INFO:lda:<1280> log likelihood: -1676432\n",
      "INFO:lda:<1290> log likelihood: -1676747\n",
      "INFO:lda:<1300> log likelihood: -1676189\n",
      "INFO:lda:<1310> log likelihood: -1676581\n",
      "INFO:lda:<1320> log likelihood: -1676599\n",
      "INFO:lda:<1330> log likelihood: -1676499\n",
      "INFO:lda:<1340> log likelihood: -1676588\n",
      "INFO:lda:<1350> log likelihood: -1676202\n",
      "INFO:lda:<1360> log likelihood: -1676486\n",
      "INFO:lda:<1370> log likelihood: -1676493\n",
      "INFO:lda:<1380> log likelihood: -1677219\n",
      "INFO:lda:<1390> log likelihood: -1676631\n",
      "INFO:lda:<1400> log likelihood: -1676793\n",
      "INFO:lda:<1410> log likelihood: -1676797\n",
      "INFO:lda:<1420> log likelihood: -1675858\n",
      "INFO:lda:<1430> log likelihood: -1676423\n",
      "INFO:lda:<1440> log likelihood: -1676901\n",
      "INFO:lda:<1450> log likelihood: -1676282\n",
      "INFO:lda:<1460> log likelihood: -1675785\n",
      "INFO:lda:<1470> log likelihood: -1676197\n",
      "INFO:lda:<1480> log likelihood: -1676179\n",
      "INFO:lda:<1490> log likelihood: -1676035\n",
      "INFO:lda:<1500> log likelihood: -1675783\n",
      "INFO:lda:<1510> log likelihood: -1676436\n",
      "INFO:lda:<1520> log likelihood: -1676268\n",
      "INFO:lda:<1530> log likelihood: -1676085\n",
      "INFO:lda:<1540> log likelihood: -1675670\n",
      "INFO:lda:<1550> log likelihood: -1676090\n",
      "INFO:lda:<1560> log likelihood: -1676314\n",
      "INFO:lda:<1570> log likelihood: -1676503\n",
      "INFO:lda:<1580> log likelihood: -1676279\n",
      "INFO:lda:<1590> log likelihood: -1676041\n",
      "INFO:lda:<1600> log likelihood: -1676352\n",
      "INFO:lda:<1610> log likelihood: -1676238\n",
      "INFO:lda:<1620> log likelihood: -1676060\n",
      "INFO:lda:<1630> log likelihood: -1675736\n",
      "INFO:lda:<1640> log likelihood: -1675906\n",
      "INFO:lda:<1650> log likelihood: -1675983\n",
      "INFO:lda:<1660> log likelihood: -1675826\n",
      "INFO:lda:<1670> log likelihood: -1675825\n",
      "INFO:lda:<1680> log likelihood: -1676280\n",
      "INFO:lda:<1690> log likelihood: -1675983\n",
      "INFO:lda:<1700> log likelihood: -1675697\n",
      "INFO:lda:<1710> log likelihood: -1675504\n",
      "INFO:lda:<1720> log likelihood: -1675841\n",
      "INFO:lda:<1730> log likelihood: -1675342\n",
      "INFO:lda:<1740> log likelihood: -1675101\n",
      "INFO:lda:<1750> log likelihood: -1675491\n",
      "INFO:lda:<1760> log likelihood: -1676038\n",
      "INFO:lda:<1770> log likelihood: -1675387\n",
      "INFO:lda:<1780> log likelihood: -1676183\n",
      "INFO:lda:<1790> log likelihood: -1676166\n",
      "INFO:lda:<1800> log likelihood: -1675855\n",
      "INFO:lda:<1810> log likelihood: -1675077\n",
      "INFO:lda:<1820> log likelihood: -1674625\n",
      "INFO:lda:<1830> log likelihood: -1675603\n",
      "INFO:lda:<1840> log likelihood: -1674941\n",
      "INFO:lda:<1850> log likelihood: -1675595\n",
      "INFO:lda:<1860> log likelihood: -1674814\n",
      "INFO:lda:<1870> log likelihood: -1675295\n",
      "INFO:lda:<1880> log likelihood: -1675517\n",
      "INFO:lda:<1890> log likelihood: -1675134\n",
      "INFO:lda:<1900> log likelihood: -1675695\n",
      "INFO:lda:<1910> log likelihood: -1675635\n",
      "INFO:lda:<1920> log likelihood: -1674922\n",
      "INFO:lda:<1930> log likelihood: -1675251\n",
      "INFO:lda:<1940> log likelihood: -1675055\n",
      "INFO:lda:<1950> log likelihood: -1675510\n",
      "INFO:lda:<1960> log likelihood: -1675938\n",
      "INFO:lda:<1970> log likelihood: -1674757\n",
      "INFO:lda:<1980> log likelihood: -1675268\n",
      "INFO:lda:<1990> log likelihood: -1676024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:<1999> log likelihood: -1675283\n"
     ]
    }
   ],
   "source": [
    "def TLDA(matrix, num_topic = 20, num_iteration = 2000):\n",
    "    model = lda.LDA(n_topics=num_topic, n_iter=num_iteration, random_state=1)\n",
    "    model.fit(main_matrix)\n",
    "    return model.doc_topic_, model.topic_word_\n",
    "user_pattern_matrix, pattern_checkin_matrix = TLDA(main_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> pattern-venue distribution & pattern-venue matrix </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home (private) : 0.7071696608149437\n"
     ]
    }
   ],
   "source": [
    "# format : {pattern_1 : {venue_category_1 : a%, venue_category_2 : b%...}, pattern_2 : {...}, ...}\n",
    "pattern_venue_distribution = defaultdict(lambda : defaultdict(lambda:0))\n",
    "\n",
    "pattern_venue_matrix = [[0.0] * number_of_venue_categories] * pattern_checkin_matrix.shape[0]\n",
    "pattern_venue_matrix = np.array(pattern_venue_matrix)\n",
    "\n",
    "for i in range(pattern_checkin_matrix.shape[0]):\n",
    "    for j in range(pattern_checkin_matrix.shape[1]):\n",
    "        pattern_venue_distribution[i][index_checkin_mapping[j][0]] += pattern_checkin_matrix[i, j]\n",
    "\n",
    "for i, venues in pattern_venue_distribution.items():\n",
    "    for venue, percentage in venues.items():\n",
    "        j = venue_index_mapping[venue]\n",
    "        pattern_venue_matrix[i, j] = percentage\n",
    "\n",
    "for v, p in pattern_venue_distribution[0].items():\n",
    "    if p > 0.1:\n",
    "        print('{} : {}'.format(v, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> pattern-hour distribution & pattern-hour matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format : {pattern_1 : {hour_1 : a%, hour_2 : b%...}, pattern_2 : {...}, ...}\n",
    "pattern_hour_distribution = defaultdict(lambda : defaultdict(lambda:0))\n",
    "\n",
    "pattern_hour_matrix = [[0.0] * 24] * pattern_checkin_matrix.shape[0]\n",
    "pattern_hour_matrix = np.array(pattern_hour_matrix)\n",
    "\n",
    "for i in range(pattern_checkin_matrix.shape[0]):\n",
    "    for j in range(pattern_checkin_matrix.shape[1]):\n",
    "        pattern_hour_distribution[i][index_checkin_mapping[j][1].split('-')[2]] += pattern_checkin_matrix[i, j]\n",
    "\n",
    "for i, hours in pattern_hour_distribution.items():\n",
    "    for hour, percentage in hours.items():\n",
    "        j = hour_index_mapping[hour]\n",
    "        pattern_hour_matrix[i, j] = percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation of TLDA</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inputs:__\n",
    "1. top venue categories `V*` for each pattern.\n",
    "2. top time periods (hours) `T*` for each pattern.\n",
    "3. all the check-in activities `SW`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__High level:__\n",
    "\n",
    "1. we firstly define a segmentation `S_{one set}` for each __top venue category__ `v*` in each pattern:\n",
    "<img src=\"one_set.png\">\n",
    "\n",
    "We use `S` to denote the set of all segmentations `S_{one set}`, and `|S|` = `Q`.\n",
    "2. For each segmentation `S_{one set}`, we calculate the normalised pointwise mutual information (NPMI) for `v*-T*` vector and `V*-T*` vector, respectively: \n",
    "<img src=\"little_w.png\">\n",
    "\n",
    "where `P (v*, t*_j )` is the probability of the co-occurrence of `v*` and `t*_j`. \n",
    "\n",
    "3. After calculating the NPMI value for each venue category, we aggregate them to obtain the jth element of the time vector of `V∗` by the following equation:\n",
    "<img src=\"big_w.png\">\n",
    "\n",
    "where `v*_i` represents the ith venue category in `V*`.\n",
    "\n",
    "4. Cosine similarity is then calculated between pairs of context vectors `w_q` and `W_q`, and then obtain the final score\n",
    "<img src=\"mq.png\">\n",
    "<img src=\"m.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pseudocode:__\n",
    "<img src=\"pseudo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Construct the V* and T* for each pattern </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foramt: {pattern_1 : [index_1, index_2, index_3, index_4, index_5], pattern_2 : [], ...}\n",
    "V_star = defaultdict(list)\n",
    "for i in range(pattern_venue_matrix.shape[0]):\n",
    "    top_five_indices = pattern_venue_matrix[i].argsort()[-5:][::-1]\n",
    "    V_star[i] = top_five_indices\n",
    "\n",
    "T_star = defaultdict(list)\n",
    "for i in range(pattern_hour_matrix.shape[0]):\n",
    "    top_five_indices = pattern_hour_matrix[i].argsort()[-5:][::-1]\n",
    "    T_star[i] = top_five_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Log based 2 function </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_2(n):\n",
    "    return (math.log(n)/math.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Cosine similarity function </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(l1, l2):\n",
    "    return (1 - spatial.distance.cosine(l1, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> NPMI Function </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkin_data : {user1 : [(venue_category_1, time_1), (venue_category_2, time_2), (venue_category_3, time_3...)], \n",
    "#                user2 : [(venue_category_13, time_13), (...) ...] ...}\n",
    "\n",
    "# time format: month-day-hour; Example: (Oct-Wed-13)\n",
    "\n",
    "# index_venue_mapping, index_hour_mapping \n",
    "\n",
    "\n",
    "def NPMI(v : \"index of venue category v\", t : \"index of hour t\", checkin_data, epsilon = 0.001, tau = 2):\n",
    "    v_sum = 0\n",
    "    t_sum = 0\n",
    "    v_t_sum = 0\n",
    "\n",
    "    for u, checkins in checkin_data.items():\n",
    "        for checkin in checkins:\n",
    "            if checkin[0] == index_venue_mapping[v]:\n",
    "                v_sum += 1\n",
    "            if checkin[1].split('-')[2] == index_hour_mapping[t]:\n",
    "                t_sum += 1\n",
    "            if checkin[0] == index_venue_mapping[v] and checkin[1].split('-')[2] == index_hour_mapping[t]:\n",
    "                v_t_sum += 1\n",
    "\n",
    "    p_v = v_sum / number_of_checkins\n",
    "    p_t = t_sum / number_of_checkins\n",
    "    p_v_t = v_t_sum / number_of_checkins\n",
    "\n",
    "    upper = (p_v_t + epsilon)/(p_v * p_t)\n",
    "    lower = p_v_t + epsilon\n",
    "\n",
    "    numerator = log_2(upper)\n",
    "    denominator = -log_2(lower)\n",
    "\n",
    "    result = (numerator / denominator)**tau\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9307714597546856\n"
     ]
    }
   ],
   "source": [
    "S = set()\n",
    "num_of_patterns = pattern_venue_matrix.shape[0]\n",
    "\n",
    "for i in range(num_of_patterns):\n",
    "    for v in V_star[i]:\n",
    "        S_oneset = (v, tuple(V_star[i]), tuple(T_star[i]))\n",
    "        S.add(S_oneset)\n",
    "\n",
    "m = []\n",
    "\n",
    "# S_oneset format: (v*, V*, T*)\n",
    "for S_oneset in S:\n",
    "    T = S_oneset[2]\n",
    "    w = list(i * 0 for i in range(len(T)))\n",
    "    W = list(i * 0 for i in range(len(T)))\n",
    "    for i, t in enumerate(T):\n",
    "        w[i] = NPMI(S_oneset[0], t, checkin_data)\n",
    "        sum = 0\n",
    "        for v in S_oneset[1]:\n",
    "            sum += NPMI(v, t, checkin_data)\n",
    "        W[i] = sum\n",
    "    m.append(cosine_sim(w, W))\n",
    "\n",
    "final_result = np.sum(m) / len(S)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
